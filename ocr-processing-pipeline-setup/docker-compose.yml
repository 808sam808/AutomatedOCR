version: "3.8"

# ──────────────────────────────────────────────────────────────────────
#  OCR Pipeline — Docker Compose
#  
#  Usage:
#    1. Copy .env.example to .env and fill in your GROQ_API_KEY
#    2. docker compose up -d
#    3. Pull the OCR model:
#       docker compose exec ollama ollama pull glm-ocr:q8_0
#    4. Drop images into ./watch_folder/
#    5. Open http://localhost:8080 to view results
# ──────────────────────────────────────────────────────────────────────

services:

  # ── Ollama (local LLM server for OCR) ────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: ocr-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # ── GPU Support (uncomment the section that matches your setup) ──
    #
    # NVIDIA GPU:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    #
    # AMD GPU (ROCm):
    # devices:
    #   - /dev/kfd
    #   - /dev/dri
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ── OCR Watcher + Web Dashboard ──────────────────────────────────
  ocr-watcher:
    build: .
    container_name: ocr-watcher
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "${WEB_PORT:-8080}:8080"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.3-70b-versatile}
      - WEB_PORT=8080
      - UPLOAD_STABLE_SECONDS=${UPLOAD_STABLE_SECONDS:-3}
    volumes:
      - ./watch_folder:/app/watch_folder
      - ./OCR:/app/OCR

  # ── (Optional) Model Puller — runs once to download the model ───
  ollama-pull:
    image: curlimages/curl:latest
    container_name: ocr-model-pull
    depends_on:
      ollama:
        condition: service_healthy
    restart: "no"
    entrypoint: >
      sh -c '
        echo "Checking if glm-ocr:q8_0 is already available…";
        if curl -sf http://ollama:11434/api/show -d "{\"name\":\"glm-ocr:q8_0\"}" > /dev/null 2>&1; then
          echo "✅ Model already available.";
        else
          echo "⬇️  Pulling glm-ocr:q8_0 (this may take a while)…";
          curl -sf http://ollama:11434/api/pull -d "{\"name\":\"glm-ocr:q8_0\",\"stream\":false}";
          echo "";
          echo "✅ Model pulled successfully.";
        fi
      '

volumes:
  ollama_data:
    driver: local
